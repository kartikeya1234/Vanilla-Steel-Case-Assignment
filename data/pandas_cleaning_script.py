import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# =============================================================================
# COMPLETE INVENTORY DATASET CLEANING & JOINING WITH PANDAS
# =============================================================================

print("üîÑ Starting complete data cleaning and joining process...\n")

# Step 1: Read original Excel files
print("üìñ Step 1: Reading original Excel files...")
try:
    dataset1_raw = pd.read_excel('supplier_data_1.xlsx')
    dataset2_raw = pd.read_excel('supplier_data_2.xlsx')
    
    print(f"   Dataset 1: {len(dataset1_raw)} records, {len(dataset1_raw.columns)} columns")
    print(f"   Dataset 2: {len(dataset2_raw)} records, {len(dataset2_raw.columns)} columns")
except Exception as e:
    print(f"‚ùå Error reading files: {e}")
    print("Make sure the Excel files are in the current directory")

# =============================================================================
# STEP 2: CLEAN DATASET 1 (German supplier data)
# =============================================================================
print("\nüßπ Step 2: Cleaning Dataset 1 (German supplier data)...")

def clean_dataset1(df):
    """Clean German supplier dataset with proper handling of missing values"""
    print("   Analyzing Dataset 1 structure...")
    
    # Make a copy to avoid modifying original
    df_clean = df.copy()
    
    # Show original missing data patterns
    missing_counts = df_clean.isnull().sum()
    print(f"   Original missing values: {missing_counts.sum()} total")
    
    # Define critical columns that must have values
    critical_columns = [
        'Nenndicke NNN.NN mm mit Dezimalpunkt',
        'Breite', 
        'L√§nge', 
        'Gewicht (kg)'
    ]
    
    # Remove rows missing critical data
    initial_count = len(df_clean)
    for col in critical_columns:
        if col in df_clean.columns:
            df_clean = df_clean.dropna(subset=[col])
    
    removed_count = initial_count - len(df_clean)
    print(f"   Removed {removed_count} records missing critical physical dimensions")
    
    # Standardize column names to English
    column_mapping = {
        'Werksg√ºte': 'material_grade',
        'Bestellg√ºtentext': 'material_name',
        'Nenndicke NNN.NN mm mit Dezimalpunkt': 'thickness_raw',
        'Breite': 'width_raw',
        'L√§nge': 'length_mm',
        'Gewicht (kg)': 'weight_kg',
        'Cluster': 'cluster',
        'Si-Gehalt': 'si_content',
        'Mn-Gehalt': 'mn_content',
        'P-Gehalt': 'p_content',
        'S-Gehalt': 's_content',
        'Cr-Gehalt': 'cr_content',
        'Ni-Gehalt': 'ni_content',
        'Mo-Gehalt': 'mo_content',
        'V-Gehalt': 'v_content',
        'Cu-Gehalt': 'cu_content',
        'Nb-Gehalt': 'nb_content',
        'Ti-Gehalt': 'ti_content',
        'Al-Gehalt': 'al_content',
        'B-Gehalt': 'b_content',
        'Streckgrenze': 'yield_strength',
        'Zugfestigkeit': 'tensile_strength',
        'Dehnung': 'elongation'
    }
    
    # Rename columns
    df_clean = df_clean.rename(columns=column_mapping)
    
    # Convert thickness from assumed 0.01mm scale to mm
    if 'thickness_raw' in df_clean.columns:
        df_clean['thickness_mm'] = df_clean['thickness_raw'] / 100.0
    
    # Clean width data (handle comma decimal separators)
    if 'width_raw' in df_clean.columns:
        df_clean['width_mm'] = pd.to_numeric(
            df_clean['width_raw'].astype(str).str.replace(',', '.'), 
            errors='coerce'
        )
    
    # Convert empty strings to NaN for proper missing value handling
    chemical_columns = [col for col in df_clean.columns if '_content' in col]
    mechanical_columns = ['yield_strength', 'tensile_strength', 'elongation']
    
    for col in chemical_columns + mechanical_columns:
        if col in df_clean.columns:
            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
    
    # Add source identifier
    df_clean['source_dataset'] = 'german_supplier'
    
    # Create unique inventory IDs
    df_clean['inventory_id'] = [f"GER_{i+1:03d}" for i in range(len(df_clean))]
    
    print(f"   Cleaned dataset 1: {len(df_clean)} records retained")
    return df_clean

dataset1_clean = clean_dataset1(dataset1_raw)

# =============================================================================
# STEP 3: CLEAN DATASET 2 (English supplier data)
# =============================================================================
print("\nüßπ Step 3: Cleaning Dataset 2 (English supplier data)...")

def clean_dataset2(df):
    """Clean English supplier dataset"""
    print("   Analyzing Dataset 2 structure...")
    
    # Make a copy
    df_clean = df.copy()
    
    # Show original missing data patterns
    missing_counts = df_clean.isnull().sum()
    print(f"   Original missing values: {missing_counts.sum()} total")
    
    # Define critical columns
    critical_columns = [
        'NOMINAL_THICKNESS_MM',
        'WIDTH_MM',
        'MASS_MIN_KG',
        'ORDER_ID'
    ]
    
    # Remove rows missing critical data
    initial_count = len(df_clean)
    for col in critical_columns:
        if col in df_clean.columns:
            df_clean = df_clean.dropna(subset=[col])
    
    removed_count = initial_count - len(df_clean)
    print(f"   Removed {removed_count} records missing critical data")
    
    # Standardize column names
    column_mapping = {
        'PRODUCT_TYPE': 'product_type',
        'ORDER_ID': 'order_id',
        'SITE': 'site',
        'MATERIAL_NAME': 'material_name',
        'MATERIAL_NUMBER': 'material_number',
        'MATERIAL_QUALITY_NORM': 'material_quality_norm',
        'SURFACE_COATING': 'surface_coating',
        'DEFECT_NOTES': 'defect_notes',
        'NOMINAL_THICKNESS_MM': 'thickness_mm',
        'WIDTH_MM': 'width_mm',
        'LENGTH_MM': 'length_mm',
        'HEIGHT_MM': 'height_mm',
        'MASS_MIN_KG': 'weight_kg',
        'NUMBER_OF_COILS': 'number_of_coils',
        'DELIVERY_EARLIEST': 'delivery_earliest',
        'DELIVERY_LATEST': 'delivery_latest',
        'INCO_TERM': 'inco_term',
        'BUY_NOW_EUR_PER_TON': 'buy_now_eur_per_ton',
        'MIN/MAX_BID_EUR_PER_TON': 'bid_eur_per_ton',
        'CO2_PER_TON_MAX_KG': 'co2_per_ton_max_kg',
        'VALID_UNTIL': 'valid_until'
    }
    
    # Rename columns
    df_clean = df_clean.rename(columns=column_mapping)
    
    # Remove columns that are completely empty
    df_clean = df_clean.dropna(axis=1, how='all')
    
    # Convert numeric columns
    numeric_columns = ['thickness_mm', 'width_mm', 'length_mm', 'weight_kg', 
                      'buy_now_eur_per_ton', 'bid_eur_per_ton']
    
    for col in numeric_columns:
        if col in df_clean.columns:
            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')
    
    # Add source identifier
    df_clean['source_dataset'] = 'english_supplier'
    
    # Create unique inventory IDs
    df_clean['inventory_id'] = [f"ENG_{i+1:03d}" for i in range(len(df_clean))]
    
    print(f"   Cleaned dataset 2: {len(df_clean)} records retained")
    return df_clean

dataset2_clean = clean_dataset2(dataset2_raw)

# =============================================================================
# STEP 4: CREATE UNIFIED SCHEMA AND JOIN DATASETS
# =============================================================================
print("\nüîó Step 4: Creating unified schema and joining datasets...")

def create_unified_inventory(df1, df2):
    """Create unified inventory dataset with consistent schema"""
    
    # Define the complete unified schema
    unified_columns = [
        'inventory_id',
        'material_grade',
        'material_name', 
        'material_number',
        'material_quality_norm',
        'thickness_mm',
        'width_mm',
        'length_mm',
        'weight_kg',
        'cluster',
        'product_type',
        'order_id',
        'site',
        'defect_notes',
        'inco_term',
        'buy_now_eur_per_ton',
        'bid_eur_per_ton',
        'valid_until',
        # Chemical composition (German dataset only)
        'si_content', 'mn_content', 'p_content', 's_content', 'cr_content',
        'ni_content', 'mo_content', 'v_content', 'cu_content', 'nb_content',
        'ti_content', 'al_content', 'b_content',
        # Mechanical properties (German dataset only)
        'yield_strength', 'tensile_strength', 'elongation',
        'source_dataset'
    ]
    
    # Prepare dataset 1 for union
    df1_unified = pd.DataFrame(index=df1.index)
    for col in unified_columns:
        if col in df1.columns:
            df1_unified[col] = df1[col]
        else:
            df1_unified[col] = np.nan
    
    # Prepare dataset 2 for union  
    df2_unified = pd.DataFrame(index=df2.index)
    for col in unified_columns:
        if col in df2.columns:
            df2_unified[col] = df2[col]
        else:
            df2_unified[col] = np.nan
    
    # Combine datasets
    inventory_dataset = pd.concat([df1_unified, df2_unified], ignore_index=True)
    
    # Reorder columns logically
    column_order = [
        'inventory_id', 'source_dataset',
        'material_grade', 'material_name', 'material_number', 'material_quality_norm',
        'thickness_mm', 'width_mm', 'length_mm', 'weight_kg',
        'cluster', 'product_type', 'order_id', 'site',
        'si_content', 'mn_content', 'p_content', 's_content', 'cr_content',
        'ni_content', 'mo_content', 'v_content', 'cu_content', 'nb_content',
        'ti_content', 'al_content', 'b_content',
        'yield_strength', 'tensile_strength', 'elongation',
        'defect_notes', 'inco_term', 'buy_now_eur_per_ton', 'bid_eur_per_ton', 'valid_until'
    ]
    
    inventory_dataset = inventory_dataset[column_order]
    
    print(f"   Created unified inventory dataset: {len(inventory_dataset)} records")
    return inventory_dataset

# Create the final unified dataset
inventory_dataset = create_unified_inventory(dataset1_clean, dataset2_clean)

# =============================================================================
# STEP 5: DATA QUALITY VALIDATION AND SUMMARY
# =============================================================================
print("\n‚úÖ Step 5: Data quality validation and summary...")

def validate_and_summarize(df):
    """Validate data quality and create summary statistics"""
    
    summary = {
        'total_records': len(df),
        'by_source': df['source_dataset'].value_counts().to_dict(),
        'missing_by_column': df.isnull().sum().to_dict(),
        'data_quality_issues': {}
    }
    
    # Check for data quality issues
    issues = []
    
    # Physical constraints validation
    if 'thickness_mm' in df.columns:
        invalid_thickness = (df['thickness_mm'] <= 0).sum()
        if invalid_thickness > 0:
            issues.append(f"Invalid thickness: {invalid_thickness} records")
    
    if 'width_mm' in df.columns:
        invalid_width = (df['width_mm'] <= 0).sum() 
        if invalid_width > 0:
            issues.append(f"Invalid width: {invalid_width} records")
    
    if 'weight_kg' in df.columns:
        invalid_weight = (df['weight_kg'] <= 0).sum()
        if invalid_weight > 0:
            issues.append(f"Invalid weight: {invalid_weight} records")
    
    summary['data_quality_issues'] = issues
    
    return summary

validation_summary = validate_and_summarize(inventory_dataset)

# =============================================================================
# STEP 6: OUTPUT RESULTS AND EXPORT
# =============================================================================
print("\nüìä FINAL RESULTS SUMMARY")
print("=" * 50)
print(f"‚úÖ Total records in inventory_dataset: {validation_summary['total_records']}")
for source, count in validation_summary['by_source'].items():
    print(f"   - {source}: {count} records")

if validation_summary['data_quality_issues']:
    print("‚ö†Ô∏è  Data quality issues found:")
    for issue in validation_summary['data_quality_issues']:
        print(f"   - {issue}")
else:
    print("‚úÖ No data quality issues detected")

print(f"\n‚úÖ Dataset successfully unified with {len(inventory_dataset.columns)} columns")
print("‚úÖ Proper NULL handling for missing values")
print("‚úÖ No artificial scaling applied - measurements preserved")
print("‚úÖ Consistent English column naming")

print("\nüìã DOCUMENTED ASSUMPTIONS")
print("=" * 50)
print("1. Thickness conversion: Dataset 1 raw values divided by 100 (assumed 0.01mm ‚Üí mm)")
print("2. Missing value strategy: Empty strings converted to NaN, preserved as NULL")
print("3. Join approach: UNION (concatenation) to preserve all records from both datasets")
print("4. Record removal: Only records missing critical dimensions (thickness, width, weight)")
print("5. Column standardization: All German column names translated to English")
print("6. Data integrity: Original measurement values preserved without normalization")
print("7. ID generation: Unique inventory_id created for each record (GER_001, ENG_001, etc.)")

print("\nüíæ Dataset Schema:")
print("=" * 50)
print(f"Columns ({len(inventory_dataset.columns)} total):")
for i, col in enumerate(inventory_dataset.columns, 1):
    missing_pct = (inventory_dataset[col].isnull().sum() / len(inventory_dataset) * 100)
    print(f"{i:2d}. {col:<25} ({missing_pct:.1f}% missing)")

print("\nüîç Sample Records:")
print("=" * 50)
print(inventory_dataset[['inventory_id', 'source_dataset', 'material_name', 
                        'thickness_mm', 'width_mm', 'weight_kg']].head(3))

print("\nüíæ Exporting to CSV...")
try:
    inventory_dataset.to_csv('inventory_dataset.csv', index=False)
    print("‚úÖ Successfully exported to 'inventory_dataset.csv'")
except Exception as e:
    print(f"‚ùå Export error: {e}")

print("\nüéâ Data cleaning and joining completed successfully!")
print("üìÅ Final dataset saved as 'inventory_dataset.csv'")
print(f"üìä Ready for analysis: {len(inventory_dataset)} records, {len(inventory_dataset.columns)} columns")

# Optional: Display basic statistics
print("\nüìà Quick Statistics:")
print("=" * 50)
numeric_cols = inventory_dataset.select_dtypes(include=[np.number]).columns
if len(numeric_cols) > 0:
    print(inventory_dataset[numeric_cols].describe())
